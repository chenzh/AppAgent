#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»ç®¡ç†å™¨
åŠŸèƒ½ï¼šæ•´åˆé…ç½®ã€çˆ¬è™«ã€æ•´ç†å’Œè¾“å‡ºåŠŸèƒ½
"""

import yaml
import json
import os
import logging
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path
from jinja2 import Template
import requests
import feedparser
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

from news_digest import NewsDigest, NewsCategory

class NewsManager:
    """æ–°é—»ç®¡ç†å™¨ä¸»ç±»"""
    
    def __init__(self, config_file: str = "news_config.yaml"):
        self.config_file = config_file
        self.config = self.load_config()
        self.digest = NewsDigest()
        self.setup_logging()
        
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'system': {
                'fetch_interval': 3600,
                'max_news_count': 50,
                'output_format': ['txt', 'json'],
                'auto_categorize': True,
                'auto_importance': True
            },
            'news_sources': {
                'rss': {
                    'people_politics': {
                        'name': 'äººæ°‘ç½‘æ”¿æ²»',
                        'url': 'http://www.people.com.cn/rss/politics.xml',
                        'enabled': True,
                        'category': 'æ”¿æ²»'
                    }
                }
            }
        }
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('logging', {})
        logging.basicConfig(
            level=getattr(logging, log_config.get('level', 'INFO')),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_config.get('file', 'news_manager.log'), encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def fetch_news(self) -> NewsDigest:
        """è·å–æ–°é—»"""
        self.logger.info("å¼€å§‹è·å–æ–°é—»...")
        
        # è·å–RSSæ–°é—»
        rss_sources = self.config.get('news_sources', {}).get('rss', {})
        for source_id, source_config in rss_sources.items():
            if source_config.get('enabled', True):
                self.logger.info(f"æ­£åœ¨è·å– {source_config['name']} çš„æ–°é—»...")
                news_list = self.fetch_rss_news(source_config['url'])
                self.process_news_list(news_list, source_config['name'])
        
        # è·å–ç½‘é¡µæ–°é—»
        web_sources = self.config.get('news_sources', {}).get('web', {})
        for source_id, source_config in web_sources.items():
            if source_config.get('enabled', True):
                self.logger.info(f"æ­£åœ¨è·å– {source_config['name']} çš„æ–°é—»...")
                news_list = self.fetch_web_news(source_config['url'], source_config.get('selectors', {}))
                self.process_news_list(news_list, source_config['name'])
        
        self.logger.info(f"å…±è·å–åˆ° {len(self.digest.news_items)} æ¡æ–°é—»")
        return self.digest
    
    def fetch_rss_news(self, rss_url: str) -> List[Dict[str, Any]]:
        """ä»RSSæºè·å–æ–°é—»"""
        try:
            feed = feedparser.parse(rss_url)
            news_list = []
            
            for entry in feed.entries[:10]:
                news = {
                    'title': entry.title,
                    'summary': entry.summary if hasattr(entry, 'summary') else '',
                    'url': entry.link,
                    'publish_time': entry.published if hasattr(entry, 'published') else '',
                    'source': feed.feed.title if hasattr(feed.feed, 'title') else 'RSSæº'
                }
                news_list.append(news)
            
            return news_list
        except Exception as e:
            self.logger.error(f"è·å–RSSæ–°é—»å¤±è´¥: {e}")
            return []
    
    def fetch_web_news(self, url: str, selectors: Dict[str, str]) -> List[Dict[str, Any]]:
        """ä»ç½‘é¡µè·å–æ–°é—»"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_list = []
            
            if 'news_container' in selectors:
                containers = soup.select(selectors['news_container'])
                for container in containers[:10]:
                    news = {}
                    
                    if 'title' in selectors:
                        title_elem = container.select_one(selectors['title'])
                        if title_elem:
                            news['title'] = title_elem.get_text(strip=True)
                    
                    if 'summary' in selectors:
                        summary_elem = container.select_one(selectors['summary'])
                        if summary_elem:
                            news['summary'] = summary_elem.get_text(strip=True)
                    
                    if 'url' in selectors:
                        url_elem = container.select_one(selectors['url'])
                        if url_elem and url_elem.get('href'):
                            news['url'] = urljoin(url, url_elem['href'])
                    
                    if 'time' in selectors:
                        time_elem = container.select_one(selectors['time'])
                        if time_elem:
                            news['publish_time'] = time_elem.get_text(strip=True)
                    
                    if news.get('title'):
                        news['source'] = urlparse(url).netloc
                        news_list.append(news)
            
            return news_list
        except Exception as e:
            self.logger.error(f"è·å–ç½‘é¡µæ–°é—»å¤±è´¥: {e}")
            return []
    
    def process_news_list(self, news_list: List[Dict[str, Any]], source_name: str):
        """å¤„ç†æ–°é—»åˆ—è¡¨"""
        for news in news_list:
            # è¿‡æ»¤æ–°é—»
            if not self.filter_news(news):
                continue
            
            # è‡ªåŠ¨åˆ†ç±»
            category = self.auto_categorize(news['title'], news['summary'])
            
            # åˆ¤æ–­é‡è¦æ€§
            importance = self.determine_importance(news['title'], news['summary'], source_name)
            
            # æ·»åŠ åˆ°æ‘˜è¦
            self.digest.add_news(
                title=news['title'],
                summary=news['summary'],
                category=category,
                source=source_name,
                url=news.get('url', ''),
                publish_time=news.get('publish_time', ''),
                importance=importance
            )
    
    def filter_news(self, news: Dict[str, Any]) -> bool:
        """è¿‡æ»¤æ–°é—»"""
        filters = self.config.get('filters', {})
        
        # æ ‡é¢˜é»‘åå•è¿‡æ»¤
        title_blacklist = filters.get('title_blacklist', [])
        if any(keyword in news['title'] for keyword in title_blacklist):
            return False
        
        # æ¥æºé»‘åå•è¿‡æ»¤
        source_blacklist = filters.get('source_blacklist', [])
        if any(keyword in news.get('source', '') for keyword in source_blacklist):
            return False
        
        # æ ‡é¢˜é•¿åº¦è¿‡æ»¤
        min_length = filters.get('min_title_length', 5)
        max_length = filters.get('max_title_length', 100)
        if not (min_length <= len(news['title']) <= max_length):
            return False
        
        # æ‘˜è¦é•¿åº¦è¿‡æ»¤
        min_summary_length = filters.get('min_summary_length', 10)
        max_summary_length = filters.get('max_summary_length', 500)
        if not (min_summary_length <= len(news['summary']) <= max_summary_length):
            return False
        
        return True
    
    def auto_categorize(self, title: str, summary: str) -> NewsCategory:
        """è‡ªåŠ¨åˆ†ç±»æ–°é—»"""
        if not self.config.get('system', {}).get('auto_categorize', True):
            return NewsCategory.SOCIETY
        
        categories_config = self.config.get('categories', {})
        text = (title + ' ' + summary).lower()
        
        for category_name, config in categories_config.items():
            keywords = config.get('keywords', [])
            if any(keyword in text for keyword in keywords):
                # å°†ä¸­æ–‡åˆ†ç±»åè½¬æ¢ä¸ºæšä¸¾
                for category_enum in NewsCategory:
                    if category_enum.value == category_name:
                        return category_enum
        
        return NewsCategory.SOCIETY
    
    def determine_importance(self, title: str, summary: str, source: str) -> int:
        """åˆ¤æ–­æ–°é—»é‡è¦æ€§"""
        if not self.config.get('system', {}).get('auto_importance', True):
            return 3
        
        importance_config = self.config.get('importance', {})
        text = (title + ' ' + summary).lower()
        
        # é‡è¦å…³é”®è¯
        critical_keywords = importance_config.get('critical_keywords', [])
        if any(keyword in text for keyword in critical_keywords):
            return 5
        
        # æƒå¨åª’ä½“
        authoritative_sources = importance_config.get('authoritative_sources', [])
        if any(source in auth_source for auth_source in authoritative_sources):
            return 4
        
        # ä¸€èˆ¬é‡è¦å…³é”®è¯
        important_keywords = importance_config.get('important_keywords', [])
        if any(keyword in text for keyword in important_keywords):
            return 3
        
        # æ ‡é¢˜é•¿åº¦åˆ¤æ–­
        title_length_threshold = importance_config.get('title_length_threshold', 20)
        if len(title) > title_length_threshold:
            return 3
        
        return 2
    
    def generate_outputs(self):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
        output_config = self.config.get('output', {})
        
        # ç”Ÿæˆæ–‡æœ¬æ ¼å¼
        if output_config.get('txt', {}).get('enabled', True):
            self.generate_txt_output()
        
        # ç”ŸæˆJSONæ ¼å¼
        if output_config.get('json', {}).get('enabled', True):
            self.generate_json_output()
        
        # ç”ŸæˆHTMLæ ¼å¼
        if output_config.get('html', {}).get('enabled', False):
            self.generate_html_output()
    
    def generate_txt_output(self):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼è¾“å‡º"""
        txt_config = self.config.get('output', {}).get('txt', {})
        filename_template = txt_config.get('filename_template', 'news_digest_{date}.txt')
        filename = filename_template.format(date=datetime.now().strftime('%Y%m%d'))
        
        self.digest.save_to_file(filename)
        self.logger.info(f"æ–‡æœ¬æ ¼å¼è¾“å‡ºå·²ä¿å­˜åˆ°: {filename}")
    
    def generate_json_output(self):
        """ç”ŸæˆJSONæ ¼å¼è¾“å‡º"""
        json_config = self.config.get('output', {}).get('json', {})
        filename_template = json_config.get('filename_template', 'news_data_{date}.json')
        filename = filename_template.format(date=datetime.now().strftime('%Y%m%d'))
        
        self.digest.export_json(filename)
        self.logger.info(f"JSONæ ¼å¼è¾“å‡ºå·²ä¿å­˜åˆ°: {filename}")
    
    def generate_html_output(self):
        """ç”ŸæˆHTMLæ ¼å¼è¾“å‡º"""
        html_config = self.config.get('output', {}).get('html', {})
        filename_template = html_config.get('filename_template', 'news_digest_{date}.html')
        filename = filename_template.format(date=datetime.now().strftime('%Y%m%d'))
        
        # è¯»å–HTMLæ¨¡æ¿
        template_file = html_config.get('template_file', 'templates/news_template.html')
        try:
            with open(template_file, 'r', encoding='utf-8') as f:
                template_content = f.read()
        except FileNotFoundError:
            self.logger.error(f"HTMLæ¨¡æ¿æ–‡ä»¶ {template_file} ä¸å­˜åœ¨")
            return
        
        # å‡†å¤‡æ¨¡æ¿æ•°æ®
        template_data = self.prepare_html_data()
        
        # æ¸²æŸ“æ¨¡æ¿
        template = Template(template_content)
        html_content = template.render(**template_data)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.logger.info(f"HTMLæ ¼å¼è¾“å‡ºå·²ä¿å­˜åˆ°: {filename}")
    
    def prepare_html_data(self) -> Dict[str, Any]:
        """å‡†å¤‡HTMLæ¨¡æ¿æ•°æ®"""
        # ç»Ÿè®¡åˆ†ç±»æ•°æ®
        category_stats = {}
        for item in self.digest.news_items:
            category = item.category.value
            category_stats[category] = category_stats.get(category, 0) + 1
        
        # æŒ‰åˆ†ç±»ç»„ç»‡æ–°é—»
        categorized_news = {}
        for item in self.digest.news_items:
            category = item.category.value
            if category not in categorized_news:
                categorized_news[category] = []
            categorized_news[category].append({
                'title': item.title,
                'summary': item.summary,
                'source': item.source,
                'publish_time': item.publish_time,
                'importance': item.importance
            })
        
        return {
            'title': f"ä¸­å›½ä»Šæ—¥æ–°é—»æ—©æŠ¥ - {self.digest.today}",
            'date': self.digest.today,
            'total_news': len(self.digest.news_items),
            'categories_count': len(category_stats),
            'top_news_count': len(self.digest.get_top_news(3)),
            'category_stats': category_stats,
            'top_news': [
                {
                    'title': item.title,
                    'summary': item.summary,
                    'source': item.source,
                    'publish_time': item.publish_time,
                    'importance': item.importance
                }
                for item in self.digest.get_top_news(3)
            ],
            'categorized_news': categorized_news,
            'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def run(self):
        """è¿è¡Œæ–°é—»ç®¡ç†å™¨"""
        self.logger.info("æ–°é—»ç®¡ç†å™¨å¯åŠ¨")
        
        try:
            # è·å–æ–°é—»
            self.fetch_news()
            
            # ç”Ÿæˆè¾“å‡º
            self.generate_outputs()
            
            # æ˜¾ç¤ºæ‘˜è¦
            print("\n" + self.digest.generate_digest())
            
            self.logger.info("æ–°é—»ç®¡ç†å™¨è¿è¡Œå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ–°é—»ç®¡ç†å™¨è¿è¡Œå¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“° ä¸­å›½ä»Šæ—¥æ–°é—»æ—©æŠ¥ç®¡ç†å™¨")
    print("=" * 50)
    
    # åˆ›å»ºæ–°é—»ç®¡ç†å™¨
    manager = NewsManager()
    
    # è¿è¡Œç®¡ç†å™¨
    manager.run()
    
    print("\nâœ… æ–°é—»æ—©æŠ¥æ•´ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()