#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»çˆ¬è™«å·¥å…·
åŠŸèƒ½ï¼šä»å¤šä¸ªæ–°é—»æºè·å–å®æ—¶æ–°é—»å¹¶è¿›è¡Œæ•´ç†
"""

import requests
import json
import time
import re
from datetime import datetime
from typing import List, Dict, Any
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import feedparser
from news_digest import NewsDigest, NewsCategory

class NewsCrawler:
    """æ–°é—»çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def get_rss_news(self, rss_url: str) -> List[Dict[str, Any]]:
        """ä»RSSæºè·å–æ–°é—»"""
        try:
            feed = feedparser.parse(rss_url)
            news_list = []
            
            for entry in feed.entries[:10]:  # è·å–å‰10æ¡æ–°é—»
                news = {
                    'title': entry.title,
                    'summary': entry.summary if hasattr(entry, 'summary') else '',
                    'url': entry.link,
                    'publish_time': entry.published if hasattr(entry, 'published') else '',
                    'source': feed.feed.title if hasattr(feed.feed, 'title') else 'RSSæº'
                }
                news_list.append(news)
                
            return news_list
        except Exception as e:
            print(f"è·å–RSSæ–°é—»å¤±è´¥: {e}")
            return []
    
    def get_web_news(self, url: str, selectors: Dict[str, str]) -> List[Dict[str, Any]]:
        """ä»ç½‘é¡µè·å–æ–°é—»"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_list = []
            
            # æ ¹æ®é€‰æ‹©å™¨æå–æ–°é—»
            if 'news_container' in selectors:
                containers = soup.select(selectors['news_container'])
                for container in containers[:10]:  # é™åˆ¶æ•°é‡
                    news = {}
                    
                    if 'title' in selectors:
                        title_elem = container.select_one(selectors['title'])
                        if title_elem:
                            news['title'] = title_elem.get_text(strip=True)
                    
                    if 'summary' in selectors:
                        summary_elem = container.select_one(selectors['summary'])
                        if summary_elem:
                            news['summary'] = summary_elem.get_text(strip=True)
                    
                    if 'url' in selectors:
                        url_elem = container.select_one(selectors['url'])
                        if url_elem and url_elem.get('href'):
                            news['url'] = urljoin(url, url_elem['href'])
                    
                    if 'time' in selectors:
                        time_elem = container.select_one(selectors['time'])
                        if time_elem:
                            news['publish_time'] = time_elem.get_text(strip=True)
                    
                    if news.get('title'):  # åªæ·»åŠ æœ‰æ ‡é¢˜çš„æ–°é—»
                        news['source'] = urlparse(url).netloc
                        news_list.append(news)
            
            return news_list
        except Exception as e:
            print(f"è·å–ç½‘é¡µæ–°é—»å¤±è´¥: {e}")
            return []

class NewsAggregator:
    """æ–°é—»èšåˆå™¨"""
    
    def __init__(self):
        self.crawler = NewsCrawler()
        self.digest = NewsDigest()
        
        # æ–°é—»æºé…ç½®
        self.news_sources = {
            'rss': {
                'people': 'http://www.people.com.cn/rss/politics.xml',  # äººæ°‘ç½‘æ”¿æ²»
                'xinhua': 'http://www.xinhuanet.com/politics/news_politics.xml',  # æ–°åç½‘æ”¿æ²»
                'cctv': 'http://news.cctv.com/2019/07/gaiban/cmsdatainterface/page/news_1.jsonp'  # å¤®è§†æ–°é—»
            },
            'web': {
                'sina': {
                    'url': 'https://news.sina.com.cn/',
                    'selectors': {
                        'news_container': '.news-item',
                        'title': 'h2 a',
                        'summary': '.summary',
                        'url': 'h2 a',
                        'time': '.time'
                    }
                }
            }
        }
    
    def categorize_news(self, title: str, summary: str) -> NewsCategory:
        """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦è‡ªåŠ¨åˆ†ç±»æ–°é—»"""
        text = (title + ' ' + summary).lower()
        
        # æ”¿æ²»ç±»å…³é”®è¯
        politics_keywords = ['å›½åŠ¡é™¢', 'æ”¿æ²»å±€', 'ä¸­å¤®', 'æ”¿åºœ', 'æ”¿ç­–', 'æ³•è§„', 'ä¼šè®®', 'é¢†å¯¼äºº', 'ä¹ è¿‘å¹³', 'æå…‹å¼º']
        if any(keyword in text for keyword in politics_keywords):
            return NewsCategory.POLITICS
        
        # ç»æµç±»å…³é”®è¯
        economy_keywords = ['ç»æµ', 'é‡‘è', 'è‚¡å¸‚', 'é“¶è¡Œ', 'å¤®è¡Œ', 'GDP', 'æŠ•èµ„', 'è´¸æ˜“', 'å‡ºå£', 'è¿›å£', 'ä¼ä¸š']
        if any(keyword in text for keyword in economy_keywords):
            return NewsCategory.ECONOMY
        
        # ç§‘æŠ€ç±»å…³é”®è¯
        tech_keywords = ['ç§‘æŠ€', 'æŠ€æœ¯', 'äº’è”ç½‘', 'AI', 'äººå·¥æ™ºèƒ½', '5G', 'èŠ¯ç‰‡', 'åˆ›æ–°', 'æ•°å­—åŒ–']
        if any(keyword in text for keyword in tech_keywords):
            return NewsCategory.TECHNOLOGY
        
        # æ•™è‚²ç±»å…³é”®è¯
        education_keywords = ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•', 'æ‹›ç”Ÿ', 'å¤§å­¦', 'åŸ¹è®­']
        if any(keyword in text for keyword in education_keywords):
            return NewsCategory.EDUCATION
        
        # ä½“è‚²ç±»å…³é”®è¯
        sports_keywords = ['ä½“è‚²', 'è¶³çƒ', 'ç¯®çƒ', 'å¥¥è¿ä¼š', 'æ¯”èµ›', 'è¿åŠ¨å‘˜', 'å† å†›', 'èµ›äº‹']
        if any(keyword in text for keyword in sports_keywords):
            return NewsCategory.SPORTS
        
        # å¥åº·ç±»å…³é”®è¯
        health_keywords = ['å¥åº·', 'åŒ»ç–—', 'åŒ»é™¢', 'ç–¾ç—…', 'ç–«è‹—', 'è¯å“', 'æ²»ç–—', 'åŒ»ç”Ÿ']
        if any(keyword in text for keyword in health_keywords):
            return NewsCategory.HEALTH
        
        # ç¯å¢ƒç±»å…³é”®è¯
        environment_keywords = ['ç¯å¢ƒ', 'ç¯ä¿', 'æ±¡æŸ“', 'æ°”å€™', 'ç”Ÿæ€', 'ç»¿è‰²', 'å¯æŒç»­å‘å±•']
        if any(keyword in text for keyword in environment_keywords):
            return NewsCategory.ENVIRONMENT
        
        # æ–‡åŒ–ç±»å…³é”®è¯
        culture_keywords = ['æ–‡åŒ–', 'è‰ºæœ¯', 'ç”µå½±', 'éŸ³ä¹', 'æ–‡å­¦', 'å†å²', 'ä¼ ç»Ÿ', 'åšç‰©é¦†']
        if any(keyword in text for keyword in culture_keywords):
            return NewsCategory.CULTURE
        
        # å›½é™…ç±»å…³é”®è¯
        international_keywords = ['å›½é™…', 'å¤–äº¤', 'ç¾å›½', 'æ—¥æœ¬', 'æ¬§æ´²', 'è”åˆå›½', 'å…¨çƒ', 'ä¸–ç•Œ']
        if any(keyword in text for keyword in international_keywords):
            return NewsCategory.INTERNATIONAL
        
        # é»˜è®¤ä¸ºç¤¾ä¼šç±»
        return NewsCategory.SOCIETY
    
    def determine_importance(self, title: str, summary: str, source: str) -> int:
        """åˆ¤æ–­æ–°é—»é‡è¦æ€§ç­‰çº§"""
        text = (title + ' ' + summary).lower()
        
        # é‡è¦å…³é”®è¯
        important_keywords = ['ç´§æ€¥', 'é‡å¤§', 'é‡è¦', 'çªå‘', 'é¦–æ¬¡', 'çªç ´', 'åˆ›æ–°', 'æ”¿ç­–', 'æ³•è§„']
        if any(keyword in text for keyword in important_keywords):
            return 5
        
        # æƒå¨åª’ä½“
        authoritative_sources = ['æ–°åç¤¾', 'äººæ°‘æ—¥æŠ¥', 'å¤®è§†', 'ä¸­å¤®', 'å›½åŠ¡é™¢']
        if any(source in authoritative_sources for source in authoritative_sources):
            return 4
        
        # ä¸€èˆ¬é‡è¦
        if len(title) > 20 or 'å‘å¸ƒ' in text or 'å…¬å¸ƒ' in text:
            return 3
        
        return 2
    
    def aggregate_news(self) -> NewsDigest:
        """èšåˆæ–°é—»"""
        print("å¼€å§‹è·å–æ–°é—»...")
        
        # ä»RSSæºè·å–æ–°é—»
        for source_name, rss_url in self.news_sources['rss'].items():
            print(f"æ­£åœ¨è·å– {source_name} çš„æ–°é—»...")
            news_list = self.crawler.get_rss_news(rss_url)
            
            for news in news_list:
                category = self.categorize_news(news['title'], news['summary'])
                importance = self.determine_importance(news['title'], news['summary'], news['source'])
                
                self.digest.add_news(
                    title=news['title'],
                    summary=news['summary'],
                    category=category,
                    source=news['source'],
                    url=news.get('url', ''),
                    publish_time=news.get('publish_time', ''),
                    importance=importance
                )
        
        # ä»ç½‘é¡µè·å–æ–°é—»
        for source_name, config in self.news_sources['web'].items():
            print(f"æ­£åœ¨è·å– {source_name} çš„æ–°é—»...")
            news_list = self.crawler.get_web_news(config['url'], config['selectors'])
            
            for news in news_list:
                category = self.categorize_news(news['title'], news['summary'])
                importance = self.determine_importance(news['title'], news['summary'], news['source'])
                
                self.digest.add_news(
                    title=news['title'],
                    summary=news['summary'],
                    category=category,
                    source=news['source'],
                    url=news.get('url', ''),
                    publish_time=news.get('publish_time', ''),
                    importance=importance
                )
        
        print(f"å…±è·å–åˆ° {len(self.digest.news_items)} æ¡æ–°é—»")
        return self.digest

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“° ä¸­å›½ä»Šæ—¥æ–°é—»æ—©æŠ¥è‡ªåŠ¨æ•´ç†å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºæ–°é—»èšåˆå™¨
    aggregator = NewsAggregator()
    
    # èšåˆæ–°é—»
    digest = aggregator.aggregate_news()
    
    # ç”Ÿæˆå¹¶æ˜¾ç¤ºæ–°é—»æ—©æŠ¥
    print("\n" + digest.generate_digest())
    
    # ä¿å­˜æ–‡ä»¶
    digest.save_to_file()
    digest.export_json()
    
    print("\nâœ… æ–°é—»æ—©æŠ¥è‡ªåŠ¨æ•´ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()